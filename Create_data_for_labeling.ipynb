{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from common_functions import *\n",
    "import datetime\n",
    "import csv\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "import datetime\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_and_subset(l, n):\n",
    "    if n > len(l):\n",
    "        print('asked for {} elements, list length = {}'.format(n, len(l)))\n",
    "    i_s = random.sample(range(len(l)), n)\n",
    "    return [[l[i] for i in i_s], [l[i] for i in range(len(l)) if i not in i_s]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = \"Transcript_Segments\"\n",
    "cnn_f = data_dir + '/CNN_jan2018_{}.csv'\n",
    "msnbc_f = data_dir + '/MSNBC_jan2018_{}.csv'\n",
    "fox_f = data_dir + '/FOXNEWS_jan2018_{}.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_data(labelers, N, a, condition_text, f_name):\n",
    "    network_fs = [msnbc_f, cnn_f, fox_f]\n",
    "    snippets_by_labeler = {labeler: [] for labeler in labelers}\n",
    "    for f in network_fs:\n",
    "        #f = data_dir + '/{}_full_segments.csv'.format(network)\n",
    "        candidates = []\n",
    "        with open(f.format('segments'),'r') as csv_f:\n",
    "            for i,row in enumerate(csv.reader(line.replace('\\0', ' ') for line in csv_f)):\n",
    "                if i==0: continue\n",
    "                if condition_text(row[7]):\n",
    "                    candidates.append(row[0])\n",
    "        if len(candidates) < len(labelers)*int(N*(1-a)) + int(N*a):\n",
    "            print(\"N too big\")\n",
    "            return\n",
    "        alpha_group, candidates = sample_and_subset(candidates, int(N*a))\n",
    "        segs_by_labeler = {labeler: alpha_group for labeler in labelers}\n",
    "        for labeler in labelers:\n",
    "            to_label, candidates = sample_and_subset(candidates, int(N*(1-a)))\n",
    "            segs_by_labeler[labeler] = segs_by_labeler[labeler] + to_label\n",
    "        # now get snippets\n",
    "        with open(f.format('snippets'),'r') as csv_f:\n",
    "            for i,row in enumerate(csv.reader(x.replace('\\0',' ') for x in csv_f)):\n",
    "                if i==0: continue\n",
    "                for labeler, segs in segs_by_labeler.items():\n",
    "                    if row[0] in segs:\n",
    "                        print(row[:2])\n",
    "                        snippets_by_labeler[labeler].append((row[0], row[1], f.split('/')[-1].split('_')[0], row[8]))\n",
    "    for labeler, snippets in snippets_by_labeler.items():\n",
    "        rows = [[\"Segment\", \"Snippet\", \"Network\", \"text\", \"ABOUT TRUMP-RUSSIA?\"]]\n",
    "        for snippet_info in snippets:\n",
    "            rows.append(list(snippet_info))\n",
    "        with open(data_dir + '/labeling_data/{}_{}_unlabeled.csv'.format(labeler, f_name),'w') as csv_f:\n",
    "            csv.writer(csv_f).writerows(rows)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_words(x):\n",
    "    for term in ['INVESTIGATE', 'BILL', 'INTELLIGENCE', 'RUSSIA', 'PUTIN', 'HACK', 'EMAILS', 'SANCTION', 'KGB', 'MUELLER', 'COMEY', 'REPORT', 'INDICT','LEAK']:\n",
    "        if term in x:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10001_4', '7']\n",
      "['10001_4', '8']\n",
      "['10001_4', '9']\n",
      "['10005_3', '7']\n",
      "['10007_0', '0']\n",
      "['10007_0', '1']\n",
      "['10007_0', '2']\n",
      "['10016_2', '6']\n",
      "['10016_2', '7']\n",
      "['10050_0', '0']\n",
      "['10011_5', '11']\n",
      "['10011_5', '12']\n",
      "['10013_1', '1']\n",
      "['10013_1', '2']\n",
      "['10013_1', '3']\n",
      "['10038_5', '10']\n",
      "['10038_5', '11']\n",
      "['10041_11', '14']\n",
      "['10041_11', '15']\n",
      "['10041_11', '16']\n",
      "['10060_5', '7']\n",
      "['10060_5', '8']\n",
      "['10006_0', '0']\n",
      "['10006_0', '1']\n",
      "['10006_3', '8']\n",
      "['10006_3', '9']\n",
      "['10014_1', '3']\n",
      "['10014_1', '4']\n",
      "['10026_0', '0']\n",
      "['10026_0', '1']\n",
      "['10026_0', '2']\n",
      "['10026_0', '3']\n",
      "['10037_3', '8']\n",
      "['10037_3', '9']\n"
     ]
    }
   ],
   "source": [
    "prepare_data(['baird'], 5, 0.0, check_words, '2018jan_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
